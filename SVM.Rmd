---
title: "M치quinas de Soporte Vectorial"
author: "Carlos A. Ar."
date: "`r format(Sys.Date(), '%d de %B del %Y')`"
output: 
  rmdformats::downcute:
    self_contained: true
    thumbnails: true
    lightbox: true
    gallery: false
    highlight: tango
---

```{=html}
<style>
  body {
    font-size: 17px;
  }
</style>
```

# 쯈u칠 es?

Es una **t칠cnica** de **aprendizaje supervisado** que sirve **para clasificar observaciones** en un data set y tiene el objetivo de predecir la clase (etiqueta) a la cual debe pertenecer una observaci칩n fuera del data set.

# 쮺칩mo funciona?

Para asignar una etiqueta a cada observaci칩n, se desea

-   Dividir el espacio predictor en dos *(o m치s)* partes. Cada parte corresponder치 a una etiqueta diferente de la variable respuesta $y$ que es categ칩rica.

-   La divisi칩n se har치 con l칤mites y m치rgenes.

    -   El c치lculo de los l칤mites y m치rgenes depende de una funci칩n llamada **Kernel**.

-   La construcci칩n de los l칤mites depender치 *de la forma* en que se encuentren las observaciones del data set.

-   Se elegir치 un "costo" que definir치 la tolerancia al error de clasificaci칩n.

# Lo matem치tico *(ideas)*

Sea $\mathbb{R}^{p+1}$ el espacio predictor *(un espacio vectorial de dimensi칩n* $p$*)*, con $p \in \{1, 2, ...\}$ no infinito.

A un vector $x$ en $\mathbb{R}^{p+1}$ le llamaremos **observaci칩n** y es tal que $x := (x_1, x_2, ...., x_p, y)$ donde $y \in \{-1, 1\}$ representa **la variable respuesta**, que es categ칩rica y sus etiquetas son dos: $-1, 1$. Las **variables explicativas** son $x_1, ..., x_p$.

Sea $n \in \mathbb{N}$. Decimos que **un dataset es** el conjunto $\{x_1, ..., x_n\}$ contenido en el espacio predictor.

Por ende, dada $i \in \{1, ..., n\}$, se tiene que

$$
x_i := (x_{i1}, x_{i2}, ..., x_{ip}, y_i) 
$$

Es posible que $x_i$ considere o no a la variable respuesta $y_i$.

## De lo simple a lo complejo

Supongamos $p = 2$. Consideremos la siguiente gr치fica:

<div>
<p style = 'text-align:center;'>
![](images/svm1.png){width='550px'}
</p>
</div>

Podemos observar que en los ejes tenemos las variables respuesta y el color distinto de los datos representa la etiqueta a la que pertenece dicha observaci칩n. Esto implica que cada observaci칩n bajo la linea es de una etiqueta y encima de otra.

Este es el caso m치s simple porque:

1.  Las observaciones se separan con una linea recta.

2.  Las observaciones se separan en conjuntos disjuntos (los rosas y los azules no se traslapan).

3.  Hay solo dos variables explicativas.

En general, no se tienen datasets tan simples y vamos a generalizar desde el 칰limo *ceteris paribus*, hasta el primer punto.

## Dimensionalidad

Supongamos que ahora tenemos m치s de dos variables explicativas, $p$.. 쯈u칠 ser칤a una l칤nea recta cuando hay m치s de 2 variables explicativas? 춰Un hiperplano!

**Hiperplano.**

Definimos el hiperplano asociado al espacio predictor $\mathbb{R}^{p+1}$ como el conjunto$H := \{(x_1, x_2, ..., x_p) \in \mathbb{R}^p \space | \space \beta_0 + \beta_1x_1 + ...+ \beta_p x_p = 0\}$ donde $\forall i \in \{0, ..., p\}, \space \beta_i \in \mathbb{R}$.

Sea $f: \mathbb{R}^p \rightarrow \mathbb{R}$ la funci칩n no nula dada por $f(x) = \beta_0 + \beta_1x_1 + ...+ \beta_p x_p$.

Para culquier $x \in \mathbb{R}^p$ se tiene una y solo una de las siguientes opciones:

$$
f(x) > 0, \rightarrow x \notin H \\
f(x) = 0, \rightarrow x \in H \\
f(x) < 0, \rightarrow x \notin H
$$

Podemos arbitrariamente decir que si $f(x) > 0$ entonces designaremos su correspondiente variable respuesta $y = 1$. An치logo que si $f(x) < 0, \rightarrow y = -1$.

Cuando tenemos un dataset y encontramos al menos un hiperplano $H$ que cumple con tales caracter칤sticas (m치s las 1 y 2), tal hiperplano es 칰til para clasificar.

Es decir que, dado un dataset de tama침o $n$, si sucede que $\forall i \in \{1, ..., n\}$

$$
y_if(x_i) > 0 
$$

entonces $H$ es un buen clasificador.

Como **no solo** nos interesa que **las observaciones de ese dataset** est칠n separadas (sino tener una buena predicci칩n), nos gustar칤a que la distancia entre las observaciones de cada etiqueta y el hiperplano, sea la m치xima posible.

Entonces, sea $d_i^{(1)}$ la distancia perpendicular de la observaci칩n $i$ al hiperplano cuando la observaci칩n tiene la etiqueta 1, esto es

$$
d_i^{(1)} := \frac{|f(x_i)|}{\sqrt{\beta_1^2 +  ... + \beta_p^2}}, y_i = 1
$$

An치logamente se define $d_i^{(-1)}$.

La distancia m치xima del hiperplano "al dataset" es

$$
m_H := \max\{d_i^{(1)}\space|\space i = 1, ...,n\} + \max\{d_i^{(-1)}\space|\space i = 1, ...,n\}
$$

<div>
<p style = 'text-align:center;'>
![](images/svm2.png){width='550px'}
</p>
</div>

Queremos encontar el hiperplano que consiga la mayor de las distancias entre este y el mismo data set. Entonces queremos

$$
M := \max\{m_H\space|\space H \text{ es hiperplano separador} \}
$$

Al hiperplano que cumple con esta caracter칤stica se le conoce como Maximal Margin Classifier porque a $M$ se le llama **m치rgen**.

<div>
<p style = 'text-align:center;'>
![](https://cdn-images-1.medium.com/max/600/0*iDZMC0LsciIANcYv.gif){width='550px'}
</p>
</div>

## Traslape

Ya que tenemos m치s dimensiones, podemos ver el ejemplo cuando:

<div>
<p style = 'text-align:center;'>
![](images/svm3.png){width='550px'}
</p>
</div>

Es claro que las observaciones no son separables por un hiperplano (que en este caso es una recta), por lo cual, si queremos clasificar con los datos que tenemos, tendremos que ser m치s flexibles.

El t칠rmino $\epsilon_i$ es introducido para contar "los errores" en t칠rminos de distancia que "se pasa al otro lado del hiperplano". Nosotros podemos definir el costo que vamos a asumir por esos errores de clasificaci칩n $C$.

Dado que ya no puede ser que todas las observaciones en el dataset sean correctamente clasficadas, debemos tratar de maximizar el m치rgen a pesar del error que nosotros deseamos asumir ($C$ es un hiperpar치metro).

Por 칰ltimo, el hiperplano tendr치 m치rgenes paralelos que calcularemos para identificar que "las observaciones dentro de los m치rgenes/l칤mites est치n en peligro de ser malclasificadas."

##### Problema de Optimizaci칩n

El problema de optimizaci칩n de forma matem치tica es el siguiente:

$$
\begin{align*}
\max\{ M \space | \space H, \epsilon_i\} \space & ... (1) \\
\sum_{j = 1} ^ p \beta_j^2 = 1 \space & ...(2) \\
y_if(x_i) \geq M(1-\epsilon_i) & \space ... (3) \\
\epsilon_i \geq 0, \sum_{i = 1} ^ n \epsilon_i \leq C \space &...(4)
\end{align*}
$$

Veamos el significado de cada parte del problema de optimizaci칩n:

1.  Es la funci칩n objetivo, queremos maximizar el margen, es decir, seguir teniendo un hiperplano lo m치s lejos posible a ambas clases.
2.  Es una restricci칩n para que se cumpla la definici칩n de hiperplano ($f$ no puede ser nula)
3.  Nos gustar칤a que est칠n bien clasificadas las observaciones y adem치s que no se pasen demasiado dentro de los l칤mites que define el margen.
4.  Los errores de clasificaci칩n deben ser positivos y no deben superar nuestro deseo de mala clasificaci칩n.

En el ejemplo de aplicaci칩n podremos ver que a mayor costo, menor margen y por ende, mayor error de clasificaci칩n.

<div>
<p style = 'text-align:center;'>
![](images/svm4.png){width='550px'}
</p>
</div>

A este hiperplano con sus m치rgenes se le llama Support Vector Classifier.

**Los vectores soporte** son squellas observaciones del data set que quedan en el borde de los l칤mites, es decir, de las l칤neas punteadas.

## L칤mites no lineales

Ahora que adminitmos m치s dimensiones y posibles errores de clasificaci칩n, consideremos la siguiente gr치fica:

<div>
<p style = 'text-align:center;'>
![](images/svm5.png){width='550px'}
</p>
</div>

Podemos notar que una l칤nea recta no funcionar칤a para hacer la clasificaci칩n. Es decir que la forma que tienen nuestros datos no nos permite considerar UN hiperplano apriori. Tenemos entonces la necesidad de introducir otro concepto llamado **Kernel**.

De la gr치fica podemos rescatar que no es posible separar los datos por medio de un hiperplano asociado a este espacio de dimensi칩n 2. Pero 쯈u칠 tal que en otro espacio, tales observaciones s칤 sean separables por un hiperplano asociado a una dimensi칩n mayor?

Para entender la idea anterior considera estas gr치ficas:

-   **Dimensi칩n** $p = 1$, es decir que solo hay una variable explicativa.

    <div>
    <p style = 'text-align:center;'>
    ![](https://miro.medium.com/max/1032/1*Fzuev5gpiFb71yEBb6EV3A.png){width='550px'}
    </p>
    </div>

-   **Dimensi칩n** $p = 2$, es decir que hay dos variables explicativas.

    <div>
    <p style = 'text-align:center;'>
    ![](https://miro.medium.com/max/1074/0*s6XNDOrhoDihmeuL){width='550px'}
    </p>
    </div>

En ambos casos se tomaron los datos que estaban en una dimensi칩n (1 칩 2), se transformaron y se "a침adi칩 una dimensi칩n" en la que las observaciones s칤 fueran separables por un hiperplano.

La pregunta ser칤a 쮺칩mo se a침ade una dimensi칩n m치s que permita tal separaci칩n? Pues la respuesta es: usa una funci칩n que "levante" las observaciones de una clase y "baje/deje igual" a las observaciones de otra clase. Esta funci칩n se llama **kernel.**

### Kernel

Sea $\phi: \mathbb{R}^p \rightarrow \mathbb{R}^r$ donde $r>p$. Decimos que $K: \mathbb{R}^{2p} \rightarrow \mathbb{R}$ es un **kernel** asociado a $\phi$ si $\forall x, x^* \in \mathbb{R}^p$ se tiene que

$$
K(x, x^*) = \phi(x)^T \phi(x^*) 
$$

Es decir que la funci칩n kernel, asigna un valor que relaciona dos vectores. Esta relaci칩n entre los vectores se hace en un espacio *vectorial* de dimensi칩n mayor en el que suponemos que se puede hacer la "divisi칩n de las clases de forma lineal".

### Ejemplos de funciones kernel

Existen funciones kernel famosas por su aplicaci칩n. Porque tales funciones permiten que los l칤mites entre las clases de observaciones, sean "flexibles".

-   Asociado al problema de Support Vector Classifier (el lineal).

    $$
    K(x_i, x_j) = \sum_{l = 1}^p x_{il}x_{jl}
    $$

    <div>
    <p style = 'text-align:center;'>
    ![](images/svm6.png){width='550px'}
    </p>
    </div>

-   Polinomial *de grado* $d$

    $$
    K(x_i, x_j) = \bigg( 1 + \sum_{l = 1}^p x_{il}x_{jl}\bigg)^d
    $$

    <div>
    <p style = 'text-align:center;'>
    ![](images/svm7.png){width='550px'}
    </p>
    </div>

-   Radial

    $$
    K(x_i, x_j) = \exp \bigg\{ -\gamma \sum_{l = 1}^p (x_{il} - x_{jl})^2\bigg\}
    $$

    <div>
    <p style = 'text-align:center;'>
    ![](images/svm8.png){width='550px'}
    </p>
    </div>

Con este concepto podremos saber la forma general y simplificada del funcionamiento de una M치quina de Soporte Vectorial (Support Vector Machine SVM).

쮺칩mo se usa el kernel en las SVM?

-   Recordemos primero que consideramos una funci칩n $f$ de la siguiente forma

    $$
    f(x) =  \beta_0 + \beta_1x_1 + ...+ \beta_p x_p := \beta_0 + \beta \cdot x $$

    Donde $\beta = (\beta_1, ..., \beta_p)$ y $\cdot$ es el producto punto/interior.

-   La anterior, pero en el contexto de dataset (es decir teniendo datos), se puede escribir como:

    $$
    f(x) = \beta_0 + \sum_{i = 1} ^ n \alpha_i \langle x, x_i\rangle
    $$

    donde $\langle x_i, x_j\rangle :=\displaystyle \sum_{l = i}^p x_{il}x_{jl}$ *(producto interno)* y $\alpha_i$ son par치metros a estimar. Los vectores que no son vectores soporte tienen valores de $\alpha = 0$ por lo cual, dado $S$ el conjunto de indices de las observaciones en el dataset que son vectores soporte, tenemos que:

    $$
    f(x) = \beta_0 + \sum_{i \in S} \alpha_i \langle x, x_i\rangle
    $$

-   Para construir una SVM, se plantea la funci칩n como:

    $$
    f(x) = \beta_0 + \sum_{i \in S} \alpha_i K(x, x_i)
    $$

Y lo anterior se contempla en el [problema de optimizaci칩n] previamente planteado.

Ahora veamos c칩mo se puede usar esta t칠cnica en la computadora:

# [Aplicaci칩n en R](https://carlos-arguello.shinyapps.io/Tutorial_SVM/)

### Referencias

-   [An Introduccion to Statistical Learning](https://www.dropbox.com/s/krvhmt7z8zxhl7f/ISLRv2_website.pdf?dl=0) with Applications in R. Second Edition.

-   [Datacamp Course. Support Vector Machine in R](https://learn.datacamp.com/courses/support-vector-machines-in-r).

-   [RDocumentation](https://www.rdocumentation.org/packages/e1071/versions/1.7-8). `e1071.`

-   [M칠todos Kernel para clasificaci칩n](https://gtas.unican.es/files/docencia/APS/apuntes/07_svm_kernel.pdf). Universidad de Cantabria.

-   [Support Vector Regression: propiedades y aplicaciones](https://idus.us.es/bitstream/handle/11441/43808/Mart%C3%ADn%20Guare%C3%B1o%2C%20Juan%20Jos%C3%A9%20TFG.pdf?sequence=1&isAllowed=y). Universidad de Sevilla

### Cont치ctame

游붚[Twitter](https://twitter.com/CarlosAAr6)

游눹[Github](https://github.com/CarlosA-Ar/SVM-con-R)
